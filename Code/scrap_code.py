# # -*- coding: utf-8 -*-
# """3_create_seq2attention_data_edited

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1FtVtGO9cfWNaIpODtRMFNP896RkSNqTV
# """

# import pandas as pd
# import numpy as np
# import sys

# def cut_sequence(ad_seq, at_seq, total_misses, x_len, y_len, customer_id, loan_id):
# 	# We need to split input data into cases that were Medium Attention vs. High Attention
# 	ids_l_M = []
# 	ids_M = []
# 	quarter_M = []
# 	xs_M = []
# 	ys_M = []

# 	ids_l_H = []
# 	ids_H = []
# 	quarter_H = []
# 	xs_H = []
# 	ys_H = []

# 	misses_M = []
# 	misses_H = [] 

# 	# under = (len(ad_seq) % 3)
# 	# if under != 0:
# 	# 	add = 3 - under

# 	# 	for i in range(add):
# 	# 		ad_seq += ad_seq[-1]
# 	# 		at_seq += at_seq[-1]

#   # need to double check this
# 	for i in range(0, len(ad_seq)-x_len-y_len, 3):

# 		quarter = i/3
# 		ad_x = ad_seq[i:i+x_len]
# 		at_x = at_seq[i:i+x_len]
# 		at_y = at_seq[i+x_len:i+x_len+y_len]
# 		ad_y = ad_seq[i+x_len:i+x_len+y_len]

# 		if (at_x[-1] == 'M'):
# 			y=''
# 			# Then attention changed from med to high
# 			# We will define a change from med->high or high->med as 1 else 0
# 			if 'H' in at_y:
# 				y='1'
# 			else:
# 				y='0'

# 			ids_l_M.append(loan_id)
# 			ids_M.append(customer_id)
# 			# time_M.append(total_t)
# 			quarter_M.append(quarter)
# 			misses_M.append(total_misses[i:i+x_len]) 

# 			arr_x = []
# 			for char in ad_x:
# 				arr_x.append(int(char))

# 			arr_y = []
# 			ad_y = ad_y + y
# 			for char in ad_y:
# 				arr_y.append(int(char))

# 			if len(arr_y) != 4:
# 				arr_y.append(int(y))

# 			xs_M.append(arr_x)
# 			ys_M.append(arr_y)

# 		# this is a High attention input -- note that right now the codebase
# 		# never actually does anything with HIGH attention patients
# 		elif (at_x[-1] == 'H'):
# 			y=''
# 			# Then attention changed from High to Mediums
# 			if 'M' in at_y:
# 				y='1'
# 			else:
# 				y='0'	
	
# 			ids_l_H.append(loan_id)
# 			ids_H.append(customer_id)
# 			# time_H.append(total_t)
# 			quarter_H.append(quarter)
# 			misses_H.append(total_misses[i:i+x_len]) 
			
# 			arr_x = []
# 			for char in ad_x:
# 				arr_x.append(int(char))

# 			arr_y = []
# 			ad_y = ad_y + y

# 			for char in ad_y:
# 				arr_y.append(int(char))

# 			if len(arr_y) != 4:
# 				arr_y.append(int(y))

# 			xs_H.append(arr_x)
# 			ys_H.append(arr_y)

# 	# if loan_id == 5464678:
# 	# 	print("Adherence Sequence: ", ad_seq)
# 	# 	print("Attention Sequence: ", at_seq)
# 	# 	print("Cummulative Misses: ", total_misses)
# 	# 	print(misses_M)
# 	# 	print(misses_H)

# 	return ids_M, ids_l_M, quarter_M, xs_M, ys_M, ids_H, ids_l_H, quarter_H, xs_H, ys_H, misses_M, misses_H

# def restructure_data(df, x_len, y_len):
	
# 	adherence_sequences = df['AdherenceSequence'].values
# 	attention_sequences = df['AttentionString'].values
# 	ids = df['SK_ID_CURR']
# 	ids_loans = df['SK_ID_BUREAU']
# 	# time = df['TimeInProgram']

# 	all_ids_M = []
# 	all_ids_l_M = []
# 	all_times_M = []
# 	all_x_M = []
# 	all_y_M = []
# 	all_misses_M = []


# 	all_ids_H = []
# 	all_ids_l_H = []
# 	all_times_H = []
# 	all_x_H = []
# 	all_y_H = []
# 	all_misses_H = []

# 	# pulls out most recent 6 for both adherence and attention
# 	for ad_sequence, at_sequence, id_i, id_bureau in zip(adherence_sequences,attention_sequences, ids, ids_loans):
# 		ad_sequence = ad_sequence[len(ad_sequence)::-1]
# 		at_sequence = at_sequence[len(at_sequence)::-1]
		
# 		# if id_bureau == 5464678:
# 		# 	print('Original: ', ad_sequence)

# 		alt_ad_sequence = [1 if x!='0' else 0 for x in ad_sequence]

# 		ad_sequence = ""
# 		for elem in alt_ad_sequence:
# 			ad_sequence += str(elem)  

# 		total_misses = np.array(alt_ad_sequence)
# 		total_misses = np.cumsum(total_misses)

# 		id_list_M, id_list_l_M, time_M, x_list_M, y_list_M, id_list_H, id_list_l_H, time_H, x_list_H, y_list_H, misses_M, misses_H = cut_sequence(ad_sequence, at_sequence, total_misses, x_len, y_len, id_i, id_bureau)

# 		all_ids_M += id_list_M
# 		all_ids_l_M += id_list_l_M
# 		all_times_M += time_M
# 		all_x_M += x_list_M
# 		all_y_M += y_list_M
# 		all_misses_M += misses_M

# 		all_ids_H += id_list_H
# 		all_ids_l_H += id_list_l_H
# 		all_times_H += time_H
# 		all_x_H += x_list_H
# 		all_y_H += y_list_H
# 		all_misses_H += misses_H
	
# 	all_ids_M = np.array(all_ids_M).reshape(-1,1)
# 	all_ids_l_M = np.array(all_ids_l_M).reshape(-1,1)
# 	all_times_M = np.array(all_times_M).reshape(-1,1)
# 	all_x_M = np.array(all_x_M)
# 	all_y_M = np.array(all_y_M)
# 	all_misses_M = np.array(all_misses_M) 

# 	x_M = np.concatenate([all_ids_M, all_ids_l_M, all_times_M, all_x_M], axis=1)
# 	print(all_y_M.shape)

# 	all_ids_H = np.array(all_ids_H).reshape(-1,1)
# 	all_ids_l_H = np.array(all_ids_l_H).reshape(-1,1)
# 	all_times_H = np.array(all_times_H).reshape(-1,1)
# 	all_x_H = np.array(all_x_H)
# 	all_y_H = np.array(all_y_H)
# 	all_misses_H = np.array(all_misses_H)  

# 	x_H = np.concatenate([all_ids_H, all_ids_l_H, all_times_H, all_x_H], axis=1)

# 	all_data_M = np.concatenate([x_M, all_misses_M, all_y_M], axis=1)
# 	all_data_H = np.concatenate([x_H, all_misses_H, all_y_H], axis=1)

# 	print('M samples:',all_data_M.shape)
# 	print('H samples:',all_data_H.shape)

# 	print("Saving...")
# 	print(all_data_M[1])

#   # make sure labels align e.g. Total_Months
# 	cols = ['SK_ID_CURR', 'SK_ID_BUREAU', 'QUARTER'] + ['x'+str(i + 1) for i in range(3)] + ['mis'+str(i + 1) for i in range(3)] + ['y'+str(i + 1) for i in range(3)] + ['label']
# 	data_M_df = pd.DataFrame(all_data_M, columns=cols)
# 	data_M_df.to_csv('data/processed/seq2attention_data_x'+str(x_len)+'y'+str(y_len)+'_M.csv',index=False)

# 	data_H_df = pd.DataFrame(all_data_H, columns=cols)
# 	data_H_df.to_csv('data/processed/seq2attention_data_x'+str(x_len)+'y'+str(y_len)+'_H.csv',index=False)


x_sequence_train_subset:  [[[ 0.          3.        ]
  [ 1.          3.        ]
  [ 1.          3.        ]
  ...
  [ 1.          3.        ]
  [ 1.          3.        ]
  [ 1.          3.        ]]

 [[ 1.          6.        ]
  [ 1.          6.        ]
  [ 1.          6.        ]
  ...
  [ 1.          7.        ]
  [ 1.          7.        ]
  [ 1.          7.        ]]

 [[ 1.         21.        ]
  [ 0.         22.        ]
  [ 1.         22.        ]
  ...
  [ 1.         22.        ]
  [ 1.         22.        ]
  [ 1.         22.        ]]

 ...

 [[ 1.          3.86989438]
  [ 1.          3.86989438]
  [ 1.          3.86989438]
  ...
  [ 0.62329813  4.24659625]
  [ 0.          5.24659625]
  [ 0.62329813  5.62329813]]

 [[ 1.          3.3235921 ]
  [ 0.66910198  3.65449012]
  [ 1.          3.65449012]
  ...
  [ 0.33089802  4.3235921 ]
  [ 0.          5.3235921 ]
  [ 0.33089802  5.99269407]]

 [[ 0.93644165  6.6355835 ]
  [ 0.06355835  7.57202515]
  [ 1.          7.57202515]
  ...
  [ 1.          7.6355835 ]
  [ 0.06355835  8.57202515]
  [ 1.          8.57202515]]]
x_static_train_subset:  [[0.27777778 0.22222222 0.88888889 0.53703704]
 [0.73148148 0.22222222 0.5462963  0.14814815]
 [0.73148148 0.75       1.         0.28703704]
 [0.73148148 0.62962963 0.         1.        ]
 [0.73148148 0.82407407 0.5462963  1.        ]
 [0.27777778 0.62962963 0.5462963  0.        ]
 [0.73148148 0.47222222 0.5462963  0.53703704]
 [0.27777778 0.62962963 0.5462963  0.        ]
 [0.27777778 0.22222222 0.88888889 0.53703704]
 [0.27777778 0.47222222 0.5462963  0.53703704]
 [0.73148148 0.22222222 0.5462963  0.14814815]
 [0.73148148 0.75       0.5462963  1.        ]
 [0.73148148 0.22222222 0.5462963  0.14814815]
 [0.27777778 0.62962963 0.5462963  0.        ]
 [0.73148148 0.75       1.         0.28703704]
 [0.         0.         0.         1.        ]
 [0.27777778 0.22222222 0.         0.53703704]
 [0.73148148 0.47222222 0.5462963  0.53703704]
 [0.27777778 1.         0.         0.53703704]
 [0.73148148 0.47222222 0.5462963  0.53703704]
 [0.         0.         0.         0.75925926]
 [0.27777778 1.         0.         0.53703704]
 [0.27777778 0.22222222 0.         0.53703704]
 [0.27777778 1.         0.         0.53703704]
 [1.         1.         0.         1.        ]
 [0.73148148 0.75       0.5462963  1.        ]
 [0.73148148 0.22222222 0.5462963  0.14814815]
 [0.         0.         0.5462963  1.        ]
 [0.27777778 0.22222222 0.88888889 0.53703704]
 [0.27777778 1.         0.         0.53703704]
 [0.         0.         0.         0.75925926]
 [0.73148148 0.47222222 0.5462963  0.14814815]
 [0.73148148 0.47222222 0.5462963  0.14814815]
 [0.73148148 0.22222222 0.5462963  0.14814815]
 [0.73148148 0.22222222 0.5462963  0.14814815]
 [0.73148148 0.82407407 0.5462963  1.        ]
 [0.73148148 0.22222222 0.5462963  0.14814815]
 [0.27777778 1.         0.         0.53703704]
 [0.27777778 0.85185185 0.         1.        ]
 [0.         0.         0.5462963  0.75925926]
 [0.73148148 0.75       0.5462963  1.        ]
 [0.73148148 0.47222222 0.5462963  0.53703704]
 [0.27777778 0.47222222 0.5462963  0.53703704]
 [0.73148148 0.47222222 0.5462963  1.        ]
 [0.27777778 1.         0.         0.53703704]
 [0.73148148 0.47222222 0.5462963  0.14814815]
 [0.27777778 0.22222222 0.88888889 0.53703704]
 [0.27777778 0.22222222 0.88888889 0.53703704]
 [0.73148148 0.75       1.         0.28703704]
 [0.73148148 0.47222222 0.5462963  0.53703704]
 [0.27777778 1.         0.         0.53703704]
 [0.27777778 0.22222222 0.88888889 0.53703704]
 [0.73148148 0.75       1.         0.28703704]
 [0.73148148 0.62962963 0.5462963  0.14814815]
 [0.73148148 0.47222222 0.5462963  1.        ]
 [0.31891945 0.2700809  0.0906796  0.51436714]
 [0.39518721 0.37796943 0.14137055 0.65684258]
 [0.43089267 0.91563057 0.18436283 0.69327672]
 [0.73148148 0.80098436 0.5462963  1.        ]
 [0.51373457 0.49670257 0.52006802 0.40702003]
 [0.483437   0.92025459 0.2476305  0.74689339]
 [0.27777778 0.36599791 0.         0.53703704]
 [0.27777778 0.9763436  0.         0.53703704]
 [0.37980085 0.35755895 0.12284411 0.64114221]
 [0.30522733 0.2586349  0.0330515  0.56504679]
 [0.68376426 0.72078538 0.95228278 0.3133302 ]
 [0.73148148 0.77978519 0.81756573 0.57371946]
 [0.27777778 0.95876696 0.04267982 0.53703704]
 [0.69133211 0.7708147  0.49795317 0.95903125]
 [0.64256333 0.69556032 0.91108185 0.33603275]
 [0.27777778 0.49609218 0.5215888  0.53703704]
 [0.73148148 0.79997553 0.5462963  1.        ]
 [0.4541221  0.90283068 0.21233296 0.71698023]
 [0.40088984 0.36543421 0.27134903 0.46919978]
 [0.73148148 0.76268611 0.92229757 0.40914085]
 [0.56962118 0.69854976 0.5462963  0.83483642]
 [0.73148148 0.75       0.87253897 0.48733294]
 [0.73148148 0.79019971 0.75377678 0.67395924]
 [0.27777778 0.25745328 0.07698638 0.53703704]
 [0.64173752 0.70502596 0.43823724 0.90842453]
 [0.31160168 0.49293073 0.5801202  0.51839938]
 [0.73148148 0.75       0.99405055 0.29638617]
 [0.73148148 0.76687076 0.5462963  1.        ]
 [0.64132344 0.64512228 0.43773866 0.908002  ]
 [0.27777778 0.2265172  0.         0.53703704]
 [0.64775565 0.69873929 0.5462963  0.91456548]
 [0.41077769 0.57536501 0.5462963  0.67275123]
 [0.60853103 0.60697601 0.39825392 0.87454036]
 [0.38155327 0.34294065 0.12495416 0.6429304 ]
 [0.69788715 0.76851116 0.92595535 0.3055482 ]
 [0.27777778 0.66141247 0.3504678  0.53703704]
 [0.330348   0.28337534 0.06329884 0.59068012]
 [0.73148148 0.81393359 0.60840678 0.90239781]
 [0.73148148 0.75       0.82908896 0.55561152]
 [0.58135182 0.83272451 0.36552793 0.84680647]
 [0.27777778 0.96645532 0.03472169 0.53703704]]
y_train_subset:  [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
# changed X and Y len to 6
X_LEN = 3
Y_LEN = 3

# Jackon's code filtered out additional samples which we don't want to filter out (see below)

#man_call_string_length_max_filter = 1000
#man_call_percent_filter = 0.5
#min_sequence_length = beginning_ignore + X_LEN + Y_LEN + end_ignore
#df = pd.read_csv('../data/intermediate_data/patient_data_attention_filtered_'+str(man_call_string_length_max_filter)+'_'+str(man_call_percent_filter)+'_'+str(min_sequence_length)+'.csv')

# read in data
df = pd.read_csv('data/intermediate_data/bureau_agg.csv')

training_info = df[['SK_ID_CURR', 'SK_ID_BUREAU','AdherenceSequence','AttentionString']]

restructure_data(training_info, X_LEN, Y_LEN)

import numpy as np

# def reverse_string(string):
# 	reversedString=''
# 	index = len(string) # calculate length of string and save in index
# 	while index > 0: 
#     	reversedString += string[index-1]
#     	index = index - 1 # decrement index
# 	return reversedString # reversed string

cols_sequence_x = ['x'+str(i) for i in range(1,4)] + ['mis'+str(i) for i in range(1,4)]
print(cols_sequence_x)
# string = 'hello'
# print(string[0])

# newstring = string[len(string)::-1]
# print(newstring)
# arr = np.array([0,1,2,3])
# print(arr)

# arr = np.flip(arr)
# print(arr)
# # Find correlations with the target and sort
# correlations = app_train.corr()['TARGET'].sort_values()

# print('Most Positive Correlations:\n', correlations.tail(29))
# print('\nMost Negative Correlations:\n', correlations.head(29))


# timer
# @contextmanager
# def timer(title):
#     t0 = time.time()
#     yield
#     print("{} - done in {:.0f}s".format(title, time.time() - t0))
    
# print('-' * 80)
# print('bureau_balance')
# bureau_balance = import_data('data/raw/bureau_balance.csv')

# print('-' * 80)
# print('bureau')
# bureau = import_data('data/raw/bureau.csv')

# #encode categorical variables
# app_test = one_hot_encoder(app_test)
# app_train = one_hot_encoder(app_train)

# # Align the training and testing data, keep only columns present in both dataframes
# train_labels = app_train['TARGET']

# app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)

# app_train['TARGET'] = train_labels

# print('Training Features shape: ', app_train.shape)
# print('Testing Features shape: ', app_test.shape)

# #search for anomalies
# for col in app_test.columns: 
#     print(col) 

# other code we could include e.g. this to reduce memory https://www.kaggle.com/gemartin/load-data-reduce-memory-usage

# # Function to calculate missing values by column# Funct 
# def missing_values_table(df):
#         # Total missing values
#         mis_val = df.isnull().sum()
        
#         # Percentage of missing values
#         mis_val_percent = 100 * df.isnull().sum() / len(df)
        
#         # Make a table with the results
#         mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
#         # Rename the columns
#         mis_val_table_ren_columns = mis_val_table.rename(
#         columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
#         # Sort the table by percentage of missing descending
#         mis_val_table_ren_columns = mis_val_table_ren_columns[
#             mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
#         '% of Total Values', ascending=False).round(1)
        
#         # Print some summary information
#         print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
#             "There are " + str(mis_val_table_ren_columns.shape[0]) +
#               " columns that have missing values.")
        
#         # Return the dataframe with missing information
#         return mis_val_table_ren_columns

# # One-hot encoding function for categorical columns 
# def one_hot_encoder(df, nan_as_category = True):
#     categorical_columns = [col for col in df.columns if df[col].dtype == 'object']
#     df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)
#     return df